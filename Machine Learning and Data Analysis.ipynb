{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "import collections\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Convolution1D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from keras.utils import np_utils\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import math\n",
    "import csv\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xTrain = np.load('xTrain.npy')\n",
    "yTrain = np.load('yTrain.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories=['Wins','PPG','PPGA','PowerConf','3PG','TOP', 'APG', 'Conference Champ', 'Tourney Conference Champ',\n",
    "            'Seed','SOS','SRS', 'Rebounds', 'Steals', 'Tourney Appearances','National Championships','Location']\n",
    "df = pd.DataFrame(xTrain, columns=categories)\n",
    "df['Result'] = pd.Series(yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wins</th>\n",
       "      <th>PPG</th>\n",
       "      <th>PPGA</th>\n",
       "      <th>PowerConf</th>\n",
       "      <th>3PG</th>\n",
       "      <th>TOP</th>\n",
       "      <th>APG</th>\n",
       "      <th>Conference Champ</th>\n",
       "      <th>Tourney Conference Champ</th>\n",
       "      <th>Seed</th>\n",
       "      <th>SOS</th>\n",
       "      <th>SRS</th>\n",
       "      <th>Rebounds</th>\n",
       "      <th>Steals</th>\n",
       "      <th>Tourney Appearances</th>\n",
       "      <th>National Championships</th>\n",
       "      <th>Location</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>15.266129</td>\n",
       "      <td>11.495392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.003456</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.231567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>13.75</td>\n",
       "      <td>17.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.672811</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-12.0</td>\n",
       "      <td>-4.501152</td>\n",
       "      <td>9.720046</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.109447</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-6.464286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-15.49</td>\n",
       "      <td>-29.52</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>2.896313</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>14.339719</td>\n",
       "      <td>0.476373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.120051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.784163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>5.76</td>\n",
       "      <td>18.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.199234</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-3.200893</td>\n",
       "      <td>1.906250</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.223214</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-3.303571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>-14.70</td>\n",
       "      <td>-20.11</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.151786</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.855556</td>\n",
       "      <td>0.596296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.718519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.159259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.37</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.944444</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Wins        PPG       PPGA  PowerConf       3PG  TOP       APG  \\\n",
       "0   7.0  15.266129  11.495392        1.0 -0.003456  0.0  6.231567   \n",
       "1 -12.0  -4.501152   9.720046       -1.0  0.109447 -0.0 -6.464286   \n",
       "2   9.0  14.339719   0.476373        1.0 -0.120051  0.0  6.784163   \n",
       "3  -5.0  -3.200893   1.906250       -1.0 -2.223214 -0.0 -3.303571   \n",
       "4   1.0  -0.855556   0.596296        0.0  0.718519  0.0 -0.159259   \n",
       "\n",
       "   Conference Champ  Tourney Conference Champ  Seed    SOS    SRS  Rebounds  \\\n",
       "0               0.0                       0.0 -22.0  13.75  17.34       0.0   \n",
       "1               0.0                       0.0  24.0 -15.49 -29.52      -0.0   \n",
       "2               0.0                       0.0 -17.0   5.76  18.88       0.0   \n",
       "3               0.0                       0.0  11.0 -14.70 -20.11      -0.0   \n",
       "4               0.0                       0.0   0.0   2.37   0.92       0.0   \n",
       "\n",
       "     Steals  Tourney Appearances  National Championships  Location  Result  \n",
       "0  4.672811                  4.0                     0.0       1.0     1.0  \n",
       "1  2.896313                -12.0                    -1.0       1.0     0.0  \n",
       "2  5.199234                 14.0                     0.0       1.0     1.0  \n",
       "3  0.151786                 -5.0                     0.0       1.0     0.0  \n",
       "4 -1.944444                 10.0                     0.0      -1.0     1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allWins = df[df['Result'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "          2.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          0.00000000e+00,   5.00000000e+00,   0.00000000e+00,\n",
       "          1.30000000e+01,   0.00000000e+00,   2.40000000e+01,\n",
       "          0.00000000e+00,   2.90000000e+01,   0.00000000e+00,\n",
       "          5.60000000e+01,   0.00000000e+00,   1.08000000e+02,\n",
       "          0.00000000e+00,   1.19000000e+02,   0.00000000e+00,\n",
       "          1.64000000e+02,   0.00000000e+00,   2.65000000e+02,\n",
       "          0.00000000e+00,   3.88000000e+02,   4.93000000e+02,\n",
       "          0.00000000e+00,   6.65000000e+02,   0.00000000e+00,\n",
       "          8.43000000e+02,   0.00000000e+00,   1.06000000e+03,\n",
       "          0.00000000e+00,   1.23500000e+03,   0.00000000e+00,\n",
       "          1.52100000e+03,   0.00000000e+00,   1.84500000e+03,\n",
       "          0.00000000e+00,   2.07300000e+03,   0.00000000e+00,\n",
       "          2.30000000e+03,   0.00000000e+00,   2.73800000e+03,\n",
       "          0.00000000e+00,   2.87500000e+03,   0.00000000e+00,\n",
       "          3.03600000e+03,   0.00000000e+00,   3.16000000e+03,\n",
       "          3.22900000e+03,   0.00000000e+00,   3.13200000e+03,\n",
       "          0.00000000e+00,   3.24000000e+03,   0.00000000e+00,\n",
       "          2.98500000e+03,   0.00000000e+00,   2.91600000e+03,\n",
       "          0.00000000e+00,   2.65700000e+03,   0.00000000e+00,\n",
       "          2.37000000e+03,   0.00000000e+00,   2.13000000e+03,\n",
       "          0.00000000e+00,   1.90100000e+03,   0.00000000e+00,\n",
       "          1.62800000e+03,   0.00000000e+00,   1.42300000e+03,\n",
       "          0.00000000e+00,   1.06300000e+03,   0.00000000e+00,\n",
       "          8.42000000e+02,   6.77000000e+02,   0.00000000e+00,\n",
       "          5.13000000e+02,   0.00000000e+00,   3.83000000e+02,\n",
       "          0.00000000e+00,   2.38000000e+02,   0.00000000e+00,\n",
       "          1.75000000e+02,   0.00000000e+00,   1.10000000e+02,\n",
       "          0.00000000e+00,   7.60000000e+01,   0.00000000e+00,\n",
       "          4.00000000e+01,   0.00000000e+00,   2.00000000e+01,\n",
       "          0.00000000e+00,   1.20000000e+01,   0.00000000e+00,\n",
       "          4.00000000e+00,   0.00000000e+00,   6.00000000e+00,\n",
       "          1.00000000e+00]),\n",
       " array([-23.  , -22.48, -21.96, -21.44, -20.92, -20.4 , -19.88, -19.36,\n",
       "        -18.84, -18.32, -17.8 , -17.28, -16.76, -16.24, -15.72, -15.2 ,\n",
       "        -14.68, -14.16, -13.64, -13.12, -12.6 , -12.08, -11.56, -11.04,\n",
       "        -10.52, -10.  ,  -9.48,  -8.96,  -8.44,  -7.92,  -7.4 ,  -6.88,\n",
       "         -6.36,  -5.84,  -5.32,  -4.8 ,  -4.28,  -3.76,  -3.24,  -2.72,\n",
       "         -2.2 ,  -1.68,  -1.16,  -0.64,  -0.12,   0.4 ,   0.92,   1.44,\n",
       "          1.96,   2.48,   3.  ,   3.52,   4.04,   4.56,   5.08,   5.6 ,\n",
       "          6.12,   6.64,   7.16,   7.68,   8.2 ,   8.72,   9.24,   9.76,\n",
       "         10.28,  10.8 ,  11.32,  11.84,  12.36,  12.88,  13.4 ,  13.92,\n",
       "         14.44,  14.96,  15.48,  16.  ,  16.52,  17.04,  17.56,  18.08,\n",
       "         18.6 ,  19.12,  19.64,  20.16,  20.68,  21.2 ,  21.72,  22.24,\n",
       "         22.76,  23.28,  23.8 ,  24.32,  24.84,  25.36,  25.88,  26.4 ,\n",
       "         26.92,  27.44,  27.96,  28.48,  29.  ]),\n",
       " <a list of 100 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSY35/4uqigjSSFI5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106834750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(allWins['Wins'], 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, on average the winners of basketball games have the following difference between themselves and the opponenet in the below categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of category Wins :  4.5340811044\n",
      "The mean of category PPG :  2.68545764392\n",
      "The mean of category PPGA :  -2.19561110293\n",
      "The mean of category PowerConf :  0.0917046715149\n",
      "The mean of category 3PG :  0.387640043536\n",
      "The mean of category TOP :  -0.0435052573977\n",
      "The mean of category APG :  1.09011469794\n",
      "The mean of category Conference Champ :  0.0176436407177\n",
      "The mean of category Tourney Conference Champ :  0.0155130214295\n",
      "The mean of category Seed :  -3.3596168407\n",
      "The mean of category SOS :  1.40347995281\n",
      "The mean of category SRS :  5.85602701132\n",
      "The mean of category Rebounds :  1.6099734698\n",
      "The mean of category Steals :  0.390964367582\n",
      "The mean of category Tourney Appearances :  2.53305981582\n",
      "The mean of category National Championships :  0.0907890334736\n",
      "The mean of category Location :  0.293409166945\n"
     ]
    }
   ],
   "source": [
    "for cat in categories:\n",
    "    print 'The mean of category', cat, ': ',(allWins[cat]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pearson correlation between the result and Wins is : (0.55163803336450568, 0.0)\n",
      "The Pearson correlation between the result and PPG is : (0.34523822230414264, 0.0)\n",
      "The Pearson correlation between the result and PPGA is : (-0.3034556123442686, 0.0)\n",
      "The Pearson correlation between the result and PowerConf is : (0.22539419215811074, 0.0)\n",
      "The Pearson correlation between the result and 3PG is : (0.14642298473285648, 0.0)\n",
      "The Pearson correlation between the result and TOP is : (-0.0077929646979420725, 0.0086339768643220165)\n",
      "The Pearson correlation between the result and APG is : (0.2235190440075864, 0.0)\n",
      "The Pearson correlation between the result and Conference Champ is : (0.11065630001203593, 3.1981147242103643e-306)\n",
      "The Pearson correlation between the result and Tourney Conference Champ is : (0.0993667099987873, 4.7524476396603651e-247)\n",
      "The Pearson correlation between the result and Seed is : (-0.36884650190422463, 0.0)\n",
      "The Pearson correlation between the result and SOS is : (0.29041688560416967, 0.0)\n",
      "The Pearson correlation between the result and SRS is : (0.53511972081821868, 0.0)\n",
      "The Pearson correlation between the result and Rebounds is : (0.15141653382632675, 0.0)\n",
      "The Pearson correlation between the result and Steals is : (0.1364765094039172, 0.0)\n",
      "The Pearson correlation between the result and Tourney Appearances is : (0.3280329128338767, 0.0)\n",
      "The Pearson correlation between the result and National Championships is : (0.13635835920020598, 0.0)\n",
      "The Pearson correlation between the result and Location is : (0.003407788588886428, 0.25080127192643203)\n"
     ]
    }
   ],
   "source": [
    "for cat in categories:\n",
    "    print 'The Pearson correlation between the result and', cat, 'is :',scipy.stats.pearsonr(df[cat], df['Result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['National Championships', 'Steals', 'APG', 'PPGA', 'Location'], \n",
       "      dtype='|S24')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories=['PPG','PPGA','PowerConf','3PG','TOP', 'APG', 'Conference Champ', 'Tourney Conference Champ',\n",
    "            'Seed','SOS','SRS', 'Rebounds', 'Steals', 'Tourney Appearances','National Championships','Location']\n",
    "np.random.choice(categories,5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "realCategories = ['Wins','PPG','PPGA','PowerConf','3PG', 'APG', 'Conference Champ', 'Tourney Conference Champ',\n",
    "            'Seed','SOS','SRS', 'Rebounds', 'Steals', 'Tourney Appearances','National Championships','Location']\n",
    "xTrain = df[categories].as_matrix()\n",
    "xTrain = xTrain.reshape((113567,4,4,1))\n",
    "xTrain.shape\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(xTrain, yTrain)\n",
    "Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "Y_test_categorical = np_utils.to_categorical(Y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 85175 samples, validate on 28392 samples\n",
      "Epoch 1/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.5073 - acc: 0.7472 - val_loss: 0.5059 - val_acc: 0.7511\n",
      "Epoch 2/1000\n",
      "85175/85175 [==============================] - 21s - loss: 0.4993 - acc: 0.7504 - val_loss: 0.4952 - val_acc: 0.7510\n",
      "Epoch 3/1000\n",
      "85175/85175 [==============================] - 21s - loss: 0.4962 - acc: 0.7522 - val_loss: 0.4912 - val_acc: 0.7531\n",
      "Epoch 4/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4928 - acc: 0.7530 - val_loss: 0.4900 - val_acc: 0.7512\n",
      "Epoch 5/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4910 - acc: 0.7535 - val_loss: 0.4908 - val_acc: 0.7517\n",
      "Epoch 6/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4902 - acc: 0.7544 - val_loss: 0.4902 - val_acc: 0.7554\n",
      "Epoch 7/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4895 - acc: 0.7552 - val_loss: 0.4875 - val_acc: 0.7538\n",
      "Epoch 8/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4894 - acc: 0.7551 - val_loss: 0.4885 - val_acc: 0.7517\n",
      "Epoch 9/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4891 - acc: 0.7553 - val_loss: 0.4868 - val_acc: 0.7531\n",
      "Epoch 10/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4883 - acc: 0.7558 - val_loss: 0.4878 - val_acc: 0.7533\n",
      "Epoch 11/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4880 - acc: 0.7562 - val_loss: 0.4880 - val_acc: 0.7544\n",
      "Epoch 12/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4874 - acc: 0.7568 - val_loss: 0.4883 - val_acc: 0.7540\n",
      "Epoch 13/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4873 - acc: 0.7556 - val_loss: 0.4935 - val_acc: 0.7538\n",
      "Epoch 14/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4864 - acc: 0.7567 - val_loss: 0.4860 - val_acc: 0.7530\n",
      "Epoch 15/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4863 - acc: 0.7568 - val_loss: 0.4865 - val_acc: 0.7534\n",
      "Epoch 16/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4860 - acc: 0.7567 - val_loss: 0.4884 - val_acc: 0.7518\n",
      "Epoch 17/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4848 - acc: 0.7580 - val_loss: 0.4873 - val_acc: 0.7532\n",
      "Epoch 18/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4849 - acc: 0.7571 - val_loss: 0.4887 - val_acc: 0.7520\n",
      "Epoch 19/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4848 - acc: 0.7577 - val_loss: 0.4925 - val_acc: 0.7517\n",
      "Epoch 20/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4842 - acc: 0.7583 - val_loss: 0.4887 - val_acc: 0.7536\n",
      "Epoch 21/1000\n",
      "85175/85175 [==============================] - 24s - loss: 0.4834 - acc: 0.7576 - val_loss: 0.4882 - val_acc: 0.7528\n",
      "Epoch 22/1000\n",
      "85175/85175 [==============================] - 24s - loss: 0.4836 - acc: 0.7589 - val_loss: 0.4890 - val_acc: 0.7515\n",
      "Epoch 23/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4834 - acc: 0.7585 - val_loss: 0.4880 - val_acc: 0.7531\n",
      "Epoch 24/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4833 - acc: 0.7584 - val_loss: 0.4917 - val_acc: 0.7539\n",
      "Epoch 25/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4829 - acc: 0.7593 - val_loss: 0.4931 - val_acc: 0.7525\n",
      "Epoch 26/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4823 - acc: 0.7596 - val_loss: 0.4895 - val_acc: 0.7520\n",
      "Epoch 27/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4821 - acc: 0.7589 - val_loss: 0.4960 - val_acc: 0.7526\n",
      "Epoch 28/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4812 - acc: 0.7597 - val_loss: 0.4903 - val_acc: 0.7539\n",
      "Epoch 29/1000\n",
      "85175/85175 [==============================] - 21s - loss: 0.4813 - acc: 0.7607 - val_loss: 0.4921 - val_acc: 0.7511\n",
      "Epoch 30/1000\n",
      "85175/85175 [==============================] - 21s - loss: 0.4813 - acc: 0.7604 - val_loss: 0.4904 - val_acc: 0.7517\n",
      "Epoch 31/1000\n",
      "85175/85175 [==============================] - 58s - loss: 0.4801 - acc: 0.7606 - val_loss: 0.4898 - val_acc: 0.7519\n",
      "Epoch 32/1000\n",
      "85175/85175 [==============================] - 25s - loss: 0.4805 - acc: 0.7610 - val_loss: 0.4916 - val_acc: 0.7522\n",
      "Epoch 33/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4798 - acc: 0.7614 - val_loss: 0.4898 - val_acc: 0.7525\n",
      "Epoch 34/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4809 - acc: 0.7604 - val_loss: 0.4919 - val_acc: 0.7521\n",
      "Epoch 35/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4795 - acc: 0.7607 - val_loss: 0.4912 - val_acc: 0.7513\n",
      "Epoch 36/1000\n",
      "85175/85175 [==============================] - 21s - loss: 0.4795 - acc: 0.7628 - val_loss: 0.4963 - val_acc: 0.7487\n",
      "Epoch 37/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4795 - acc: 0.7617 - val_loss: 0.4910 - val_acc: 0.7503\n",
      "Epoch 38/1000\n",
      "85175/85175 [==============================] - 21s - loss: 0.4787 - acc: 0.7626 - val_loss: 0.4904 - val_acc: 0.7513\n",
      "Epoch 39/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4782 - acc: 0.7628 - val_loss: 0.4914 - val_acc: 0.7504\n",
      "Epoch 40/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4784 - acc: 0.7626 - val_loss: 0.4916 - val_acc: 0.7515\n",
      "Epoch 41/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4782 - acc: 0.7624 - val_loss: 0.4942 - val_acc: 0.7522\n",
      "Epoch 42/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4777 - acc: 0.7623 - val_loss: 0.4943 - val_acc: 0.7476\n",
      "Epoch 43/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4775 - acc: 0.7625 - val_loss: 0.4938 - val_acc: 0.7504\n",
      "Epoch 44/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4779 - acc: 0.7633 - val_loss: 0.4928 - val_acc: 0.7512\n",
      "Epoch 45/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4765 - acc: 0.7631 - val_loss: 0.4992 - val_acc: 0.7518\n",
      "Epoch 46/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4769 - acc: 0.7640 - val_loss: 0.4950 - val_acc: 0.7501\n",
      "Epoch 47/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4761 - acc: 0.7648 - val_loss: 0.4946 - val_acc: 0.7505\n",
      "Epoch 48/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4758 - acc: 0.7641 - val_loss: 0.4934 - val_acc: 0.7496\n",
      "Epoch 49/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4753 - acc: 0.7658 - val_loss: 0.4959 - val_acc: 0.7482\n",
      "Epoch 50/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4751 - acc: 0.7654 - val_loss: 0.4955 - val_acc: 0.7477\n",
      "Epoch 51/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4754 - acc: 0.7648 - val_loss: 0.4970 - val_acc: 0.7471\n",
      "Epoch 52/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4753 - acc: 0.7658 - val_loss: 0.4966 - val_acc: 0.7483\n",
      "Epoch 53/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4747 - acc: 0.7666 - val_loss: 0.4950 - val_acc: 0.7482\n",
      "Epoch 54/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4738 - acc: 0.7658 - val_loss: 0.4974 - val_acc: 0.7480\n",
      "Epoch 55/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4738 - acc: 0.7670 - val_loss: 0.4941 - val_acc: 0.7474\n",
      "Epoch 56/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4738 - acc: 0.7665 - val_loss: 0.4978 - val_acc: 0.7476\n",
      "Epoch 57/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4731 - acc: 0.7669 - val_loss: 0.4978 - val_acc: 0.7484\n",
      "Epoch 58/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4738 - acc: 0.7658 - val_loss: 0.4954 - val_acc: 0.7482\n",
      "Epoch 59/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4734 - acc: 0.7667 - val_loss: 0.4965 - val_acc: 0.7479\n",
      "Epoch 60/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4723 - acc: 0.7682 - val_loss: 0.4988 - val_acc: 0.7484\n",
      "Epoch 61/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4721 - acc: 0.7678 - val_loss: 0.4958 - val_acc: 0.7466\n",
      "Epoch 62/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4724 - acc: 0.7680 - val_loss: 0.4979 - val_acc: 0.7484\n",
      "Epoch 63/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4721 - acc: 0.7678 - val_loss: 0.5028 - val_acc: 0.7465\n",
      "Epoch 64/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4720 - acc: 0.7682 - val_loss: 0.4985 - val_acc: 0.7473\n",
      "Epoch 65/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4713 - acc: 0.7682 - val_loss: 0.5033 - val_acc: 0.7467\n",
      "Epoch 66/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4715 - acc: 0.7681 - val_loss: 0.5012 - val_acc: 0.7450\n",
      "Epoch 67/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4714 - acc: 0.7689 - val_loss: 0.5009 - val_acc: 0.7454\n",
      "Epoch 68/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4707 - acc: 0.7697 - val_loss: 0.4975 - val_acc: 0.7469\n",
      "Epoch 69/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4709 - acc: 0.7683 - val_loss: 0.5001 - val_acc: 0.7482\n",
      "Epoch 70/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4704 - acc: 0.7698 - val_loss: 0.4994 - val_acc: 0.7469\n",
      "Epoch 71/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4695 - acc: 0.7693 - val_loss: 0.4992 - val_acc: 0.7464\n",
      "Epoch 72/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4693 - acc: 0.7696 - val_loss: 0.5011 - val_acc: 0.7470\n",
      "Epoch 73/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4710 - acc: 0.7689 - val_loss: 0.5032 - val_acc: 0.7461\n",
      "Epoch 74/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4693 - acc: 0.7697 - val_loss: 0.5047 - val_acc: 0.7473\n",
      "Epoch 75/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4688 - acc: 0.7711 - val_loss: 0.5038 - val_acc: 0.7441\n",
      "Epoch 76/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4691 - acc: 0.7698 - val_loss: 0.5013 - val_acc: 0.7486\n",
      "Epoch 77/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4678 - acc: 0.7705 - val_loss: 0.5000 - val_acc: 0.7472\n",
      "Epoch 78/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4688 - acc: 0.7700 - val_loss: 0.5023 - val_acc: 0.7464\n",
      "Epoch 79/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4676 - acc: 0.7698 - val_loss: 0.4998 - val_acc: 0.7451\n",
      "Epoch 80/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4670 - acc: 0.7710 - val_loss: 0.5052 - val_acc: 0.7454\n",
      "Epoch 81/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4683 - acc: 0.7702 - val_loss: 0.5079 - val_acc: 0.7426\n",
      "Epoch 82/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4682 - acc: 0.7700 - val_loss: 0.5076 - val_acc: 0.7467\n",
      "Epoch 83/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4670 - acc: 0.7708 - val_loss: 0.5060 - val_acc: 0.7473\n",
      "Epoch 84/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4672 - acc: 0.7705 - val_loss: 0.5060 - val_acc: 0.7444\n",
      "Epoch 85/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4667 - acc: 0.7709 - val_loss: 0.5060 - val_acc: 0.7446\n",
      "Epoch 86/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4671 - acc: 0.7721 - val_loss: 0.5102 - val_acc: 0.7431\n",
      "Epoch 87/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4669 - acc: 0.7729 - val_loss: 0.5069 - val_acc: 0.7435\n",
      "Epoch 88/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4656 - acc: 0.7734 - val_loss: 0.5065 - val_acc: 0.7438\n",
      "Epoch 89/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4647 - acc: 0.7729 - val_loss: 0.5082 - val_acc: 0.7447\n",
      "Epoch 90/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4649 - acc: 0.7724 - val_loss: 0.5073 - val_acc: 0.7427\n",
      "Epoch 91/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4663 - acc: 0.7724 - val_loss: 0.5057 - val_acc: 0.7421\n",
      "Epoch 92/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4656 - acc: 0.7724 - val_loss: 0.5055 - val_acc: 0.7436\n",
      "Epoch 93/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4660 - acc: 0.7714 - val_loss: 0.5073 - val_acc: 0.7457\n",
      "Epoch 94/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4648 - acc: 0.7737 - val_loss: 0.5060 - val_acc: 0.7439\n",
      "Epoch 95/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4654 - acc: 0.7731 - val_loss: 0.5085 - val_acc: 0.7447\n",
      "Epoch 96/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4658 - acc: 0.7727 - val_loss: 0.5082 - val_acc: 0.7458\n",
      "Epoch 97/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4629 - acc: 0.7743 - val_loss: 0.5153 - val_acc: 0.7451\n",
      "Epoch 98/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4646 - acc: 0.7730 - val_loss: 0.5146 - val_acc: 0.7430\n",
      "Epoch 99/1000\n",
      "85175/85175 [==============================] - 24s - loss: 0.4640 - acc: 0.7738 - val_loss: 0.5071 - val_acc: 0.7432\n",
      "Epoch 100/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4641 - acc: 0.7736 - val_loss: 0.5101 - val_acc: 0.7438\n",
      "Epoch 101/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4650 - acc: 0.7730 - val_loss: 0.5073 - val_acc: 0.7432\n",
      "Epoch 102/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4635 - acc: 0.7737 - val_loss: 0.5134 - val_acc: 0.7438\n",
      "Epoch 103/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4638 - acc: 0.7744 - val_loss: 0.5115 - val_acc: 0.7430\n",
      "Epoch 104/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4630 - acc: 0.7745 - val_loss: 0.5161 - val_acc: 0.7426\n",
      "Epoch 105/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4624 - acc: 0.7741 - val_loss: 0.5112 - val_acc: 0.7444\n",
      "Epoch 106/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4620 - acc: 0.7755 - val_loss: 0.5185 - val_acc: 0.7430\n",
      "Epoch 107/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4627 - acc: 0.7743 - val_loss: 0.5155 - val_acc: 0.7430\n",
      "Epoch 108/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4626 - acc: 0.7749 - val_loss: 0.5115 - val_acc: 0.7445\n",
      "Epoch 109/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4617 - acc: 0.7750 - val_loss: 0.5149 - val_acc: 0.7398\n",
      "Epoch 110/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4618 - acc: 0.7759 - val_loss: 0.5117 - val_acc: 0.7420\n",
      "Epoch 111/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4615 - acc: 0.7745 - val_loss: 0.5165 - val_acc: 0.7413\n",
      "Epoch 112/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4615 - acc: 0.7752 - val_loss: 0.5157 - val_acc: 0.7419\n",
      "Epoch 113/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4609 - acc: 0.7756 - val_loss: 0.5157 - val_acc: 0.7428\n",
      "Epoch 114/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4611 - acc: 0.7745 - val_loss: 0.5138 - val_acc: 0.7406\n",
      "Epoch 115/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4611 - acc: 0.7745 - val_loss: 0.5121 - val_acc: 0.7427\n",
      "Epoch 116/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4608 - acc: 0.7772 - val_loss: 0.5131 - val_acc: 0.7428\n",
      "Epoch 117/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4604 - acc: 0.7763 - val_loss: 0.5127 - val_acc: 0.7419\n",
      "Epoch 118/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4614 - acc: 0.7757 - val_loss: 0.5206 - val_acc: 0.7415\n",
      "Epoch 119/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4601 - acc: 0.7767 - val_loss: 0.5161 - val_acc: 0.7415\n",
      "Epoch 120/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4605 - acc: 0.7762 - val_loss: 0.5175 - val_acc: 0.7416\n",
      "Epoch 121/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4599 - acc: 0.7760 - val_loss: 0.5179 - val_acc: 0.7434\n",
      "Epoch 122/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4599 - acc: 0.7765 - val_loss: 0.5157 - val_acc: 0.7406\n",
      "Epoch 123/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4605 - acc: 0.7771 - val_loss: 0.5159 - val_acc: 0.7420\n",
      "Epoch 124/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4603 - acc: 0.7778 - val_loss: 0.5224 - val_acc: 0.7400\n",
      "Epoch 125/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4597 - acc: 0.7765 - val_loss: 0.5168 - val_acc: 0.7429\n",
      "Epoch 126/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4592 - acc: 0.7767 - val_loss: 0.5195 - val_acc: 0.7425\n",
      "Epoch 127/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4590 - acc: 0.7768 - val_loss: 0.5195 - val_acc: 0.7414\n",
      "Epoch 128/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4591 - acc: 0.7773 - val_loss: 0.5164 - val_acc: 0.7420\n",
      "Epoch 129/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4577 - acc: 0.7783 - val_loss: 0.5174 - val_acc: 0.7388\n",
      "Epoch 130/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4582 - acc: 0.7781 - val_loss: 0.5224 - val_acc: 0.7374\n",
      "Epoch 131/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4598 - acc: 0.7775 - val_loss: 0.5238 - val_acc: 0.7372\n",
      "Epoch 132/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4585 - acc: 0.7781 - val_loss: 0.5197 - val_acc: 0.7407\n",
      "Epoch 133/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4592 - acc: 0.7787 - val_loss: 0.5244 - val_acc: 0.7413\n",
      "Epoch 134/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4583 - acc: 0.7783 - val_loss: 0.5179 - val_acc: 0.7397\n",
      "Epoch 135/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4580 - acc: 0.7777 - val_loss: 0.5177 - val_acc: 0.7412\n",
      "Epoch 136/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4579 - acc: 0.7768 - val_loss: 0.5180 - val_acc: 0.7395\n",
      "Epoch 137/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4580 - acc: 0.7776 - val_loss: 0.5228 - val_acc: 0.7387\n",
      "Epoch 138/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4578 - acc: 0.7790 - val_loss: 0.5221 - val_acc: 0.7398\n",
      "Epoch 139/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4572 - acc: 0.7789 - val_loss: 0.5262 - val_acc: 0.7363\n",
      "Epoch 140/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4579 - acc: 0.7788 - val_loss: 0.5206 - val_acc: 0.7398\n",
      "Epoch 141/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4580 - acc: 0.7781 - val_loss: 0.5148 - val_acc: 0.7431\n",
      "Epoch 142/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4563 - acc: 0.7795 - val_loss: 0.5238 - val_acc: 0.7393\n",
      "Epoch 143/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4572 - acc: 0.7778 - val_loss: 0.5212 - val_acc: 0.7411\n",
      "Epoch 144/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4562 - acc: 0.7787 - val_loss: 0.5191 - val_acc: 0.7404\n",
      "Epoch 145/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4562 - acc: 0.7793 - val_loss: 0.5231 - val_acc: 0.7382\n",
      "Epoch 146/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4551 - acc: 0.7805 - val_loss: 0.5225 - val_acc: 0.7408\n",
      "Epoch 147/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4547 - acc: 0.7793 - val_loss: 0.5306 - val_acc: 0.7419\n",
      "Epoch 148/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4566 - acc: 0.7785 - val_loss: 0.5221 - val_acc: 0.7380\n",
      "Epoch 149/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4556 - acc: 0.7793 - val_loss: 0.5268 - val_acc: 0.7407\n",
      "Epoch 150/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4565 - acc: 0.7782 - val_loss: 0.5268 - val_acc: 0.7372\n",
      "Epoch 151/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4566 - acc: 0.7784 - val_loss: 0.5336 - val_acc: 0.7391\n",
      "Epoch 152/1000\n",
      "85175/85175 [==============================] - 24s - loss: 0.4561 - acc: 0.7791 - val_loss: 0.5248 - val_acc: 0.7392\n",
      "Epoch 153/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4566 - acc: 0.7776 - val_loss: 0.5266 - val_acc: 0.7421\n",
      "Epoch 154/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4554 - acc: 0.7792 - val_loss: 0.5263 - val_acc: 0.7374\n",
      "Epoch 155/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4542 - acc: 0.7797 - val_loss: 0.5331 - val_acc: 0.7384\n",
      "Epoch 156/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4550 - acc: 0.7808 - val_loss: 0.5207 - val_acc: 0.7388\n",
      "Epoch 157/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4550 - acc: 0.7791 - val_loss: 0.5244 - val_acc: 0.7407\n",
      "Epoch 158/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4544 - acc: 0.7792 - val_loss: 0.5272 - val_acc: 0.7362\n",
      "Epoch 159/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4540 - acc: 0.7811 - val_loss: 0.5311 - val_acc: 0.7401\n",
      "Epoch 160/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4554 - acc: 0.7789 - val_loss: 0.5297 - val_acc: 0.7395\n",
      "Epoch 161/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4546 - acc: 0.7803 - val_loss: 0.5270 - val_acc: 0.7369\n",
      "Epoch 162/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4547 - acc: 0.7800 - val_loss: 0.5279 - val_acc: 0.7381\n",
      "Epoch 163/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4547 - acc: 0.7811 - val_loss: 0.5236 - val_acc: 0.7384\n",
      "Epoch 164/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4552 - acc: 0.7787 - val_loss: 0.5285 - val_acc: 0.7325\n",
      "Epoch 165/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4543 - acc: 0.7802 - val_loss: 0.5210 - val_acc: 0.7383\n",
      "Epoch 166/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4545 - acc: 0.7805 - val_loss: 0.5285 - val_acc: 0.7370\n",
      "Epoch 167/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4551 - acc: 0.7792 - val_loss: 0.5298 - val_acc: 0.7365\n",
      "Epoch 168/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4530 - acc: 0.7811 - val_loss: 0.5268 - val_acc: 0.7376\n",
      "Epoch 169/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4542 - acc: 0.7800 - val_loss: 0.5233 - val_acc: 0.7404\n",
      "Epoch 170/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4542 - acc: 0.7793 - val_loss: 0.5194 - val_acc: 0.7369\n",
      "Epoch 171/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4536 - acc: 0.7808 - val_loss: 0.5280 - val_acc: 0.7369\n",
      "Epoch 172/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4533 - acc: 0.7817 - val_loss: 0.5296 - val_acc: 0.7351\n",
      "Epoch 173/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4533 - acc: 0.7815 - val_loss: 0.5271 - val_acc: 0.7372\n",
      "Epoch 174/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4519 - acc: 0.7815 - val_loss: 0.5360 - val_acc: 0.7375\n",
      "Epoch 175/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4534 - acc: 0.7819 - val_loss: 0.5306 - val_acc: 0.7368\n",
      "Epoch 176/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4529 - acc: 0.7807 - val_loss: 0.5303 - val_acc: 0.7390\n",
      "Epoch 177/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4536 - acc: 0.7817 - val_loss: 0.5310 - val_acc: 0.7379\n",
      "Epoch 178/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4539 - acc: 0.7812 - val_loss: 0.5276 - val_acc: 0.7378\n",
      "Epoch 179/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4537 - acc: 0.7821 - val_loss: 0.5300 - val_acc: 0.7366\n",
      "Epoch 180/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4535 - acc: 0.7806 - val_loss: 0.5275 - val_acc: 0.7383\n",
      "Epoch 181/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4524 - acc: 0.7814 - val_loss: 0.5280 - val_acc: 0.7363\n",
      "Epoch 182/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4526 - acc: 0.7813 - val_loss: 0.5391 - val_acc: 0.7406\n",
      "Epoch 183/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4527 - acc: 0.7817 - val_loss: 0.5315 - val_acc: 0.7350\n",
      "Epoch 184/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4541 - acc: 0.7814 - val_loss: 0.5361 - val_acc: 0.7370\n",
      "Epoch 185/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4527 - acc: 0.7811 - val_loss: 0.5338 - val_acc: 0.7381\n",
      "Epoch 186/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4522 - acc: 0.7815 - val_loss: 0.5373 - val_acc: 0.7370\n",
      "Epoch 187/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4519 - acc: 0.7815 - val_loss: 0.5347 - val_acc: 0.7385\n",
      "Epoch 188/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4526 - acc: 0.7822 - val_loss: 0.5351 - val_acc: 0.7360\n",
      "Epoch 189/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4510 - acc: 0.7830 - val_loss: 0.5421 - val_acc: 0.7365\n",
      "Epoch 190/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4537 - acc: 0.7804 - val_loss: 0.5464 - val_acc: 0.7386\n",
      "Epoch 191/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4521 - acc: 0.7828 - val_loss: 0.5319 - val_acc: 0.7368\n",
      "Epoch 192/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4516 - acc: 0.7822 - val_loss: 0.5379 - val_acc: 0.7373\n",
      "Epoch 193/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4516 - acc: 0.7820 - val_loss: 0.5302 - val_acc: 0.7383\n",
      "Epoch 194/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4530 - acc: 0.7827 - val_loss: 0.5350 - val_acc: 0.7364\n",
      "Epoch 195/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4513 - acc: 0.7836 - val_loss: 0.5327 - val_acc: 0.7361\n",
      "Epoch 196/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4529 - acc: 0.7813 - val_loss: 0.5330 - val_acc: 0.7380\n",
      "Epoch 197/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4527 - acc: 0.7825 - val_loss: 0.5323 - val_acc: 0.7390\n",
      "Epoch 198/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4529 - acc: 0.7824 - val_loss: 0.5316 - val_acc: 0.7392\n",
      "Epoch 199/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4507 - acc: 0.7837 - val_loss: 0.5359 - val_acc: 0.7384\n",
      "Epoch 200/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4526 - acc: 0.7816 - val_loss: 0.5389 - val_acc: 0.7358\n",
      "Epoch 201/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4525 - acc: 0.7823 - val_loss: 0.5373 - val_acc: 0.7386\n",
      "Epoch 202/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4510 - acc: 0.7824 - val_loss: 0.5312 - val_acc: 0.7365\n",
      "Epoch 203/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4512 - acc: 0.7830 - val_loss: 0.5380 - val_acc: 0.7371\n",
      "Epoch 204/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4510 - acc: 0.7828 - val_loss: 0.5377 - val_acc: 0.7374\n",
      "Epoch 205/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4509 - acc: 0.7826 - val_loss: 0.5380 - val_acc: 0.7380\n",
      "Epoch 206/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4503 - acc: 0.7831 - val_loss: 0.5392 - val_acc: 0.7367\n",
      "Epoch 207/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4507 - acc: 0.7840 - val_loss: 0.5393 - val_acc: 0.7370\n",
      "Epoch 208/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4512 - acc: 0.7834 - val_loss: 0.5327 - val_acc: 0.7369\n",
      "Epoch 209/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4497 - acc: 0.7840 - val_loss: 0.5403 - val_acc: 0.7357\n",
      "Epoch 210/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4518 - acc: 0.7836 - val_loss: 0.5417 - val_acc: 0.7361\n",
      "Epoch 211/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4497 - acc: 0.7837 - val_loss: 0.5375 - val_acc: 0.7344\n",
      "Epoch 212/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4509 - acc: 0.7827 - val_loss: 0.5359 - val_acc: 0.7328\n",
      "Epoch 213/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4512 - acc: 0.7832 - val_loss: 0.5375 - val_acc: 0.7373\n",
      "Epoch 214/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4508 - acc: 0.7825 - val_loss: 0.5334 - val_acc: 0.7388\n",
      "Epoch 215/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4516 - acc: 0.7828 - val_loss: 0.5314 - val_acc: 0.7383\n",
      "Epoch 216/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4506 - acc: 0.7837 - val_loss: 0.5373 - val_acc: 0.7344\n",
      "Epoch 217/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4516 - acc: 0.7818 - val_loss: 0.5339 - val_acc: 0.7363\n",
      "Epoch 218/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4508 - acc: 0.7830 - val_loss: 0.5351 - val_acc: 0.7360\n",
      "Epoch 219/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4517 - acc: 0.7826 - val_loss: 0.5294 - val_acc: 0.7363\n",
      "Epoch 220/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4510 - acc: 0.7832 - val_loss: 0.5419 - val_acc: 0.7343\n",
      "Epoch 221/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4504 - acc: 0.7825 - val_loss: 0.5442 - val_acc: 0.7367\n",
      "Epoch 222/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4528 - acc: 0.7818 - val_loss: 0.5282 - val_acc: 0.7358\n",
      "Epoch 223/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4504 - acc: 0.7820 - val_loss: 0.5341 - val_acc: 0.7347\n",
      "Epoch 224/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4503 - acc: 0.7838 - val_loss: 0.5421 - val_acc: 0.7377\n",
      "Epoch 225/1000\n",
      "85175/85175 [==============================] - 24s - loss: 0.4511 - acc: 0.7825 - val_loss: 0.5439 - val_acc: 0.7380\n",
      "Epoch 226/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4505 - acc: 0.7826 - val_loss: 0.5426 - val_acc: 0.7394\n",
      "Epoch 227/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4503 - acc: 0.7829 - val_loss: 0.5412 - val_acc: 0.7350\n",
      "Epoch 228/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4500 - acc: 0.7837 - val_loss: 0.5316 - val_acc: 0.7380\n",
      "Epoch 229/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4512 - acc: 0.7835 - val_loss: 0.5376 - val_acc: 0.7356\n",
      "Epoch 230/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4508 - acc: 0.7835 - val_loss: 0.5353 - val_acc: 0.7373\n",
      "Epoch 231/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4491 - acc: 0.7840 - val_loss: 0.5299 - val_acc: 0.7367\n",
      "Epoch 232/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4493 - acc: 0.7842 - val_loss: 0.5460 - val_acc: 0.7381\n",
      "Epoch 233/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4503 - acc: 0.7838 - val_loss: 0.5485 - val_acc: 0.7367\n",
      "Epoch 234/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4491 - acc: 0.7834 - val_loss: 0.5353 - val_acc: 0.7355\n",
      "Epoch 235/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4499 - acc: 0.7848 - val_loss: 0.5341 - val_acc: 0.7347\n",
      "Epoch 236/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4500 - acc: 0.7842 - val_loss: 0.5397 - val_acc: 0.7352\n",
      "Epoch 237/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4478 - acc: 0.7840 - val_loss: 0.5517 - val_acc: 0.7360\n",
      "Epoch 238/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4504 - acc: 0.7841 - val_loss: 0.5459 - val_acc: 0.7358\n",
      "Epoch 239/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4513 - acc: 0.7835 - val_loss: 0.5482 - val_acc: 0.7350\n",
      "Epoch 240/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4494 - acc: 0.7837 - val_loss: 0.5393 - val_acc: 0.7370\n",
      "Epoch 241/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4492 - acc: 0.7835 - val_loss: 0.5419 - val_acc: 0.7351\n",
      "Epoch 242/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4492 - acc: 0.7848 - val_loss: 0.5377 - val_acc: 0.7368\n",
      "Epoch 243/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4491 - acc: 0.7850 - val_loss: 0.5411 - val_acc: 0.7356\n",
      "Epoch 244/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4497 - acc: 0.7834 - val_loss: 0.5378 - val_acc: 0.7341\n",
      "Epoch 245/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4509 - acc: 0.7829 - val_loss: 0.5412 - val_acc: 0.7370\n",
      "Epoch 246/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4514 - acc: 0.7831 - val_loss: 0.5432 - val_acc: 0.7380\n",
      "Epoch 247/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4496 - acc: 0.7842 - val_loss: 0.5427 - val_acc: 0.7365\n",
      "Epoch 248/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4485 - acc: 0.7848 - val_loss: 0.5472 - val_acc: 0.7372\n",
      "Epoch 249/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4488 - acc: 0.7847 - val_loss: 0.5428 - val_acc: 0.7362\n",
      "Epoch 250/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4481 - acc: 0.7847 - val_loss: 0.5452 - val_acc: 0.7366\n",
      "Epoch 251/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4499 - acc: 0.7846 - val_loss: 0.5333 - val_acc: 0.7372\n",
      "Epoch 252/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4496 - acc: 0.7836 - val_loss: 0.5287 - val_acc: 0.7352\n",
      "Epoch 253/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4480 - acc: 0.7851 - val_loss: 0.5369 - val_acc: 0.7361\n",
      "Epoch 254/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4495 - acc: 0.7850 - val_loss: 0.5481 - val_acc: 0.7345\n",
      "Epoch 255/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4505 - acc: 0.7837 - val_loss: 0.5390 - val_acc: 0.7349\n",
      "Epoch 256/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4507 - acc: 0.7838 - val_loss: 0.5407 - val_acc: 0.7344\n",
      "Epoch 257/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4495 - acc: 0.7847 - val_loss: 0.5415 - val_acc: 0.7373\n",
      "Epoch 258/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4499 - acc: 0.7824 - val_loss: 0.5398 - val_acc: 0.7362\n",
      "Epoch 259/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4484 - acc: 0.7843 - val_loss: 0.5474 - val_acc: 0.7352\n",
      "Epoch 260/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4493 - acc: 0.7848 - val_loss: 0.5474 - val_acc: 0.7340\n",
      "Epoch 261/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4494 - acc: 0.7848 - val_loss: 0.5418 - val_acc: 0.7343\n",
      "Epoch 262/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4501 - acc: 0.7847 - val_loss: 0.5394 - val_acc: 0.7369\n",
      "Epoch 263/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4490 - acc: 0.7843 - val_loss: 0.5664 - val_acc: 0.7372\n",
      "Epoch 264/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4484 - acc: 0.7846 - val_loss: 0.5447 - val_acc: 0.7364\n",
      "Epoch 265/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4471 - acc: 0.7864 - val_loss: 0.5431 - val_acc: 0.7364\n",
      "Epoch 266/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4475 - acc: 0.7857 - val_loss: 0.5352 - val_acc: 0.7358\n",
      "Epoch 267/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4479 - acc: 0.7841 - val_loss: 0.5511 - val_acc: 0.7363\n",
      "Epoch 268/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4512 - acc: 0.7831 - val_loss: 0.5355 - val_acc: 0.7366\n",
      "Epoch 269/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4492 - acc: 0.7839 - val_loss: 0.5472 - val_acc: 0.7381\n",
      "Epoch 270/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4475 - acc: 0.7862 - val_loss: 0.5389 - val_acc: 0.7353\n",
      "Epoch 271/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4479 - acc: 0.7839 - val_loss: 0.5410 - val_acc: 0.7370\n",
      "Epoch 272/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4486 - acc: 0.7848 - val_loss: 0.5459 - val_acc: 0.7345\n",
      "Epoch 273/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4474 - acc: 0.7857 - val_loss: 0.5372 - val_acc: 0.7355\n",
      "Epoch 274/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4472 - acc: 0.7851 - val_loss: 0.5504 - val_acc: 0.7340\n",
      "Epoch 275/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4486 - acc: 0.7852 - val_loss: 0.5409 - val_acc: 0.7357\n",
      "Epoch 276/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4481 - acc: 0.7866 - val_loss: 0.5543 - val_acc: 0.7378\n",
      "Epoch 277/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4478 - acc: 0.7851 - val_loss: 0.5559 - val_acc: 0.7345\n",
      "Epoch 479/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4503 - acc: 0.7840 - val_loss: 0.5429 - val_acc: 0.7381\n",
      "Epoch 480/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4502 - acc: 0.7851 - val_loss: 0.5442 - val_acc: 0.7374\n",
      "Epoch 485/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4474 - acc: 0.7854 - val_loss: 0.5476 - val_acc: 0.7372\n",
      "Epoch 490/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4492 - acc: 0.7844 - val_loss: 0.5449 - val_acc: 0.7374\n",
      "Epoch 491/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4499 - acc: 0.7852 - val_loss: 0.5471 - val_acc: 0.7363\n",
      "Epoch 496/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4501 - acc: 0.7841 - val_loss: 0.5490 - val_acc: 0.7346\n",
      "Epoch 501/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4476 - acc: 0.7868 - val_loss: 0.5465 - val_acc: 0.7367\n",
      "Epoch 502/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4501 - acc: 0.7839 - val_loss: 0.5426 - val_acc: 0.7373\n",
      "Epoch 503/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4495 - acc: 0.7848 - val_loss: 0.5496 - val_acc: 0.7373\n",
      "Epoch 511/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4488 - acc: 0.7838 - val_loss: 0.5483 - val_acc: 0.7380\n",
      "Epoch 512/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4482 - acc: 0.7857 - val_loss: 0.5408 - val_acc: 0.7331\n",
      "Epoch 516/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4488 - acc: 0.7851 - val_loss: 0.5537 - val_acc: 0.7361\n",
      "Epoch 517/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4497 - acc: 0.7843 - val_loss: 0.5337 - val_acc: 0.7383\n",
      "Epoch 522/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4519 - acc: 0.7838 - val_loss: 0.5532 - val_acc: 0.7366\n",
      "Epoch 523/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4503 - acc: 0.7847 - val_loss: 0.5387 - val_acc: 0.7404\n",
      "Epoch 524/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4478 - acc: 0.7855 - val_loss: 0.5512 - val_acc: 0.7356\n",
      "Epoch 533/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4506 - acc: 0.7837 - val_loss: 0.5436 - val_acc: 0.7388\n",
      "Epoch 538/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4496 - acc: 0.7840 - val_loss: 0.5430 - val_acc: 0.7384\n",
      "Epoch 539/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4501 - acc: 0.7860 - val_loss: 0.5471 - val_acc: 0.7378\n",
      "Epoch 544/1000\n",
      "85175/85175 [==============================] - 24s - loss: 0.4502 - acc: 0.7853 - val_loss: 0.5506 - val_acc: 0.7374\n",
      "Epoch 545/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4507 - acc: 0.7860 - val_loss: 0.5493 - val_acc: 0.7342\n",
      "Epoch 549/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4504 - acc: 0.7834 - val_loss: 0.5474 - val_acc: 0.7383\n",
      "Epoch 550/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4502 - acc: 0.7851 - val_loss: 0.5513 - val_acc: 0.7381\n",
      "Epoch 555/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4506 - acc: 0.7840 - val_loss: 0.5461 - val_acc: 0.7381\n",
      "Epoch 556/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4520 - acc: 0.7845 - val_loss: 0.5487 - val_acc: 0.7359\n",
      "Epoch 562/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4491 - acc: 0.7854 - val_loss: 0.5424 - val_acc: 0.7396\n",
      "Epoch 568/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4518 - acc: 0.7845 - val_loss: 0.5440 - val_acc: 0.7394\n",
      "Epoch 574/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4508 - acc: 0.7847 - val_loss: 0.5642 - val_acc: 0.7368\n",
      "Epoch 580/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4518 - acc: 0.7847 - val_loss: 0.5428 - val_acc: 0.7368\n",
      "Epoch 586/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4514 - acc: 0.7834 - val_loss: 0.5494 - val_acc: 0.7386\n",
      "Epoch 592/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4517 - acc: 0.7837 - val_loss: 0.5489 - val_acc: 0.7377\n",
      "Epoch 598/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4521 - acc: 0.7820 - val_loss: 0.5323 - val_acc: 0.7383\n",
      "Epoch 604/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4514 - acc: 0.7828 - val_loss: 0.5369 - val_acc: 0.7355\n",
      "Epoch 610/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4516 - acc: 0.7842 - val_loss: 0.5424 - val_acc: 0.7390\n",
      "Epoch 611/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4538 - acc: 0.7837 - val_loss: 0.5383 - val_acc: 0.7347\n",
      "Epoch 616/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4518 - acc: 0.7846 - val_loss: 0.5451 - val_acc: 0.7361\n",
      "Epoch 617/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4515 - acc: 0.7839 - val_loss: 0.5341 - val_acc: 0.7393\n",
      "Epoch 622/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4551 - acc: 0.7828 - val_loss: 0.5496 - val_acc: 0.7341\n",
      "Epoch 623/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4536 - acc: 0.7836 - val_loss: 0.5417 - val_acc: 0.7365\n",
      "Epoch 628/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4546 - acc: 0.7828 - val_loss: 0.5374 - val_acc: 0.7365\n",
      "Epoch 629/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4524 - acc: 0.7835 - val_loss: 0.5384 - val_acc: 0.7389\n",
      "Epoch 634/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4521 - acc: 0.7827 - val_loss: 0.5432 - val_acc: 0.7347\n",
      "Epoch 635/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4548 - acc: 0.7823 - val_loss: 0.5398 - val_acc: 0.7389\n",
      "Epoch 640/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4529 - acc: 0.7837 - val_loss: 0.5428 - val_acc: 0.7384\n",
      "Epoch 641/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4524 - acc: 0.7828 - val_loss: 0.5429 - val_acc: 0.7358\n",
      "Epoch 647/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4521 - acc: 0.7826 - val_loss: 0.5354 - val_acc: 0.7383\n",
      "Epoch 653/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4549 - acc: 0.7826 - val_loss: 0.5345 - val_acc: 0.7372\n",
      "Epoch 654/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4532 - acc: 0.7844 - val_loss: 0.5473 - val_acc: 0.7371\n",
      "Epoch 659/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4552 - acc: 0.7828 - val_loss: 0.5398 - val_acc: 0.7394\n",
      "Epoch 660/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4560 - acc: 0.7825 - val_loss: 0.5306 - val_acc: 0.7362\n",
      "Epoch 666/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4553 - acc: 0.7822 - val_loss: 0.5420 - val_acc: 0.7395\n",
      "Epoch 672/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4521 - acc: 0.7839 - val_loss: 0.5324 - val_acc: 0.7396\n",
      "Epoch 678/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4543 - acc: 0.7829 - val_loss: 0.5414 - val_acc: 0.7367\n",
      "Epoch 684/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4536 - acc: 0.7830 - val_loss: 0.5359 - val_acc: 0.7384\n",
      "Epoch 685/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4551 - acc: 0.7828 - val_loss: 0.5362 - val_acc: 0.7409\n",
      "Epoch 690/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4548 - acc: 0.7817 - val_loss: 0.5348 - val_acc: 0.7375\n",
      "Epoch 691/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4580 - acc: 0.7816 - val_loss: 0.5387 - val_acc: 0.7391\n",
      "Epoch 697/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4553 - acc: 0.7813 - val_loss: 0.5364 - val_acc: 0.7399\n",
      "Epoch 703/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4560 - acc: 0.7816 - val_loss: 0.5321 - val_acc: 0.7372\n",
      "Epoch 704/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4573 - acc: 0.7823 - val_loss: 0.5444 - val_acc: 0.7375\n",
      "Epoch 710/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4546 - acc: 0.7814 - val_loss: 0.5457 - val_acc: 0.7400\n",
      "Epoch 716/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4578 - acc: 0.7825 - val_loss: 0.5342 - val_acc: 0.7357\n",
      "Epoch 717/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4539 - acc: 0.7810 - val_loss: 0.5376 - val_acc: 0.7340\n",
      "Epoch 722/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4594 - acc: 0.7800 - val_loss: 0.5264 - val_acc: 0.7380\n",
      "Epoch 723/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4584 - acc: 0.7819 - val_loss: 0.5230 - val_acc: 0.7378\n",
      "Epoch 729/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4563 - acc: 0.7818 - val_loss: 0.5389 - val_acc: 0.7381\n",
      "Epoch 730/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4565 - acc: 0.7814 - val_loss: 0.5357 - val_acc: 0.7360\n",
      "Epoch 736/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4568 - acc: 0.7802 - val_loss: 0.5374 - val_acc: 0.7388\n",
      "Epoch 742/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4579 - acc: 0.7811 - val_loss: 0.5326 - val_acc: 0.7373\n",
      "Epoch 743/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4565 - acc: 0.7806 - val_loss: 0.5306 - val_acc: 0.7369\n",
      "Epoch 749/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4605 - acc: 0.7793 - val_loss: 0.5261 - val_acc: 0.7379\n",
      "Epoch 750/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4570 - acc: 0.7823 - val_loss: 0.5361 - val_acc: 0.7382\n",
      "Epoch 755/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4560 - acc: 0.7820 - val_loss: 0.5330 - val_acc: 0.7347\n",
      "Epoch 756/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4572 - acc: 0.7809 - val_loss: 0.5384 - val_acc: 0.7381\n",
      "Epoch 762/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4559 - acc: 0.7815 - val_loss: 0.5293 - val_acc: 0.7383\n",
      "Epoch 763/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4583 - acc: 0.7821 - val_loss: 0.5321 - val_acc: 0.7381\n",
      "Epoch 769/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4585 - acc: 0.7804 - val_loss: 0.5226 - val_acc: 0.7400\n",
      "Epoch 770/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4535 - acc: 0.7818 - val_loss: 0.5311 - val_acc: 0.7398\n",
      "Epoch 776/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4573 - acc: 0.7809 - val_loss: 0.5364 - val_acc: 0.7328\n",
      "Epoch 777/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4578 - acc: 0.7806 - val_loss: 0.5300 - val_acc: 0.7393\n",
      "Epoch 783/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4588 - acc: 0.7794 - val_loss: 0.5317 - val_acc: 0.7368\n",
      "Epoch 790/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4599 - acc: 0.7786 - val_loss: 0.5252 - val_acc: 0.7368\n",
      "Epoch 797/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4589 - acc: 0.7804 - val_loss: 0.5297 - val_acc: 0.7370\n",
      "Epoch 804/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4577 - acc: 0.7798 - val_loss: 0.5434 - val_acc: 0.7381\n",
      "Epoch 811/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4589 - acc: 0.7799 - val_loss: 0.5332 - val_acc: 0.7369\n",
      "Epoch 818/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4597 - acc: 0.7788 - val_loss: 0.5292 - val_acc: 0.7360\n",
      "Epoch 825/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4601 - acc: 0.7776 - val_loss: 0.5277 - val_acc: 0.7368\n",
      "Epoch 826/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4609 - acc: 0.7789 - val_loss: 0.5244 - val_acc: 0.7416\n",
      "Epoch 832/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4596 - acc: 0.7794 - val_loss: 0.5286 - val_acc: 0.7385\n",
      "Epoch 839/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4640 - acc: 0.7774 - val_loss: 0.5186 - val_acc: 0.7394\n",
      "Epoch 846/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4622 - acc: 0.7787 - val_loss: 0.5244 - val_acc: 0.7402\n",
      "Epoch 853/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4589 - acc: 0.7810 - val_loss: 0.5282 - val_acc: 0.7399\n",
      "Epoch 854/1000\n",
      "85175/85175 [==============================] - 24s - loss: 0.4616 - acc: 0.7781 - val_loss: 0.5221 - val_acc: 0.7405\n",
      "Epoch 860/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4607 - acc: 0.7781 - val_loss: 0.5164 - val_acc: 0.7392\n",
      "Epoch 861/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4607 - acc: 0.7796 - val_loss: 0.5219 - val_acc: 0.7406\n",
      "Epoch 867/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4606 - acc: 0.7788 - val_loss: 0.5227 - val_acc: 0.7369\n",
      "Epoch 868/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4641 - acc: 0.7787 - val_loss: 0.5203 - val_acc: 0.7409\n",
      "Epoch 874/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4609 - acc: 0.7796 - val_loss: 0.5228 - val_acc: 0.7368\n",
      "Epoch 875/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4599 - acc: 0.7784 - val_loss: 0.5508 - val_acc: 0.7392\n",
      "Epoch 881/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4620 - acc: 0.7791 - val_loss: 0.5301 - val_acc: 0.7395\n",
      "Epoch 882/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4610 - acc: 0.7790 - val_loss: 0.5296 - val_acc: 0.7399\n",
      "Epoch 888/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4609 - acc: 0.7792 - val_loss: 0.5210 - val_acc: 0.7403\n",
      "Epoch 889/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4609 - acc: 0.7807 - val_loss: 0.5173 - val_acc: 0.7395\n",
      "Epoch 895/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4615 - acc: 0.7790 - val_loss: 0.5307 - val_acc: 0.7358\n",
      "Epoch 896/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4621 - acc: 0.7764 - val_loss: 0.5294 - val_acc: 0.7401\n",
      "Epoch 903/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4603 - acc: 0.7787 - val_loss: 0.5257 - val_acc: 0.7385\n",
      "Epoch 904/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4624 - acc: 0.7785 - val_loss: 0.5275 - val_acc: 0.7414\n",
      "Epoch 910/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4630 - acc: 0.7769 - val_loss: 0.5330 - val_acc: 0.7393\n",
      "Epoch 911/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4632 - acc: 0.7793 - val_loss: 0.5407 - val_acc: 0.7406\n",
      "Epoch 918/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4622 - acc: 0.7787 - val_loss: 0.5402 - val_acc: 0.7404\n",
      "Epoch 925/1000\n",
      "85175/85175 [==============================] - 23s - loss: 0.4649 - acc: 0.7752 - val_loss: 0.5209 - val_acc: 0.7395\n",
      "Epoch 926/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4647 - acc: 0.7779 - val_loss: 0.5295 - val_acc: 0.7406\n",
      "Epoch 933/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4633 - acc: 0.7763 - val_loss: 0.5150 - val_acc: 0.7393\n",
      "Epoch 940/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4612 - acc: 0.7768 - val_loss: 0.5410 - val_acc: 0.7380\n",
      "Epoch 941/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4636 - acc: 0.7778 - val_loss: 0.5214 - val_acc: 0.7413\n",
      "Epoch 948/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4782 - acc: 0.7679 - val_loss: 0.5193 - val_acc: 0.7394\n",
      "Epoch 954/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4626 - acc: 0.7772 - val_loss: 0.5148 - val_acc: 0.7387\n",
      "Epoch 955/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4649 - acc: 0.7759 - val_loss: 0.5256 - val_acc: 0.7395\n",
      "Epoch 961/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4635 - acc: 0.7769 - val_loss: 0.5159 - val_acc: 0.7404\n",
      "Epoch 962/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4647 - acc: 0.7756 - val_loss: 0.5208 - val_acc: 0.7387\n",
      "Epoch 968/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4603 - acc: 0.7784 - val_loss: 0.5209 - val_acc: 0.7407\n",
      "Epoch 974/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4636 - acc: 0.7753 - val_loss: 0.5223 - val_acc: 0.7379\n",
      "Epoch 975/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4661 - acc: 0.7763 - val_loss: 0.5174 - val_acc: 0.7407\n",
      "Epoch 981/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4637 - acc: 0.7760 - val_loss: 0.5268 - val_acc: 0.7395\n",
      "Epoch 982/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4646 - acc: 0.7767 - val_loss: 0.5207 - val_acc: 0.7402\n",
      "Epoch 989/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4652 - acc: 0.7770 - val_loss: 0.5226 - val_acc: 0.7420\n",
      "Epoch 994/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4658 - acc: 0.7770 - val_loss: 0.5168 - val_acc: 0.7426\n",
      "Epoch 995/1000\n",
      "85175/85175 [==============================] - 22s - loss: 0.4644 - acc: 0.7770 - val_loss: 0.5229 - val_acc: 0.7392\n"
     ]
    }
   ],
   "source": [
    "trainingLoss = []\n",
    "validationLoss = []\n",
    "img_channels = 1\n",
    "img_rows = 4\n",
    "img_cols = 4\n",
    "num_classes = 2\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 2, 2, border_mode='same', input_shape=(img_rows, img_cols, img_channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "hist = model.fit(X_train, Y_train, batch_size=32, nb_epoch=1000,shuffle=True, validation_data=(X_test, Y_test_categorical))\n",
    "trainingLoss.append(hist.history['loss'])\n",
    "validationLoss.append(hist.history['val_loss'])\n",
    "#preds = model.predict_classes( X_test, batch_size=32, verbose=1)\n",
    "#print (np.mean(preds == Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingLoss\n",
    "yRange = range(len(trainingLoss[0]))\n",
    "yRange = [x+1 for x in yRange]\n",
    "line1 = plt.plot(yRange, trainingLoss[0], label='Training Loss')\n",
    "line2 = plt.plot(yRange, validationLoss[0], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63567, 18)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain = df.as_matrix()\n",
    "xTrain = xTrain[50000:, :]\n",
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Done\n",
      "Iteration 1 Done\n",
      "Iteration 2 Done\n",
      "Iteration 3 Done\n",
      "Iteration 4 Done\n",
      "Iteration 5 Done\n",
      "Iteration 6 Done\n",
      "Iteration 7 Done\n",
      "Iteration 8 Done\n",
      "Iteration 9 Done\n",
      "Iteration 10 Done\n",
      "Iteration 11 Done\n",
      "Iteration 12 Done\n",
      "Iteration 13 Done\n",
      "Iteration 14 Done\n",
      "Iteration 15 Done\n",
      "Iteration 16 Done\n",
      "Iteration 17 Done\n",
      "Iteration 18 Done\n",
      "Iteration 19 Done\n",
      "Iteration 20 Done\n",
      "Iteration 21 Done\n",
      "Iteration 22 Done\n",
      "Iteration 23 Done\n",
      "Iteration 24 Done\n",
      "Iteration 25 Done\n",
      "Iteration 26 Done\n",
      "Iteration 27 Done\n",
      "Iteration 28 Done\n",
      "Iteration 29 Done\n",
      "Iteration 30 Done\n",
      "Iteration 31 Done\n",
      "Iteration 32 Done\n",
      "Iteration 33 Done\n",
      "Iteration 34 Done\n",
      "Iteration 35 Done\n",
      "Iteration 36 Done\n",
      "Iteration 37 Done\n",
      "Iteration 38 Done\n",
      "Iteration 39 Done\n",
      "Iteration 40 Done\n",
      "Iteration 41 Done\n",
      "Iteration 42 Done\n",
      "Iteration 43 Done\n",
      "Iteration 44 Done\n",
      "Iteration 45 Done\n",
      "Iteration 46 Done\n",
      "Iteration 47 Done\n",
      "Iteration 48 Done\n",
      "Iteration 49 Done\n",
      "Iteration 50 Done\n",
      "Iteration 51 Done\n",
      "Iteration 52 Done\n",
      "Iteration 53 Done\n",
      "Iteration 54 Done\n",
      "Iteration 55 Done\n",
      "Iteration 56 Done\n",
      "Iteration 57 Done\n",
      "Iteration 58 Done\n",
      "Iteration 59 Done\n",
      "Iteration 60 Done\n",
      "Iteration 61 Done\n",
      "Iteration 62 Done\n",
      "Iteration 63 Done\n",
      "Iteration 64 Done\n",
      "Iteration 65 Done\n",
      "Iteration 66 Done\n",
      "Iteration 67 Done\n",
      "Iteration 68 Done\n",
      "Iteration 69 Done\n",
      "Iteration 70 Done\n",
      "Iteration 71 Done\n",
      "Iteration 72 Done\n",
      "Iteration 73 Done\n",
      "Iteration 74 Done\n",
      "Iteration 75 Done\n",
      "Iteration 76 Done\n",
      "Iteration 77 Done\n",
      "Iteration 78 Done\n",
      "Iteration 79 Done\n",
      "Iteration 80 Done\n",
      "Iteration 81 Done\n",
      "Iteration 82 Done\n",
      "Iteration 83 Done\n",
      "Iteration 84 Done\n",
      "Iteration 85 Done\n",
      "Iteration 86 Done\n",
      "Iteration 87 Done\n",
      "Iteration 88 Done\n",
      "Iteration 89 Done\n",
      "Iteration 90 Done\n",
      "Iteration 91 Done\n",
      "Iteration 92 Done\n",
      "Iteration 93 Done\n",
      "Iteration 94 Done\n",
      "Iteration 95 Done\n",
      "Iteration 96 Done\n",
      "Iteration 97 Done\n",
      "Iteration 98 Done\n",
      "Iteration 99 Done\n",
      "Iteration 100 Done\n",
      "Iteration 101 Done\n",
      "Iteration 102 Done\n",
      "Iteration 103 Done\n",
      "Iteration 104 Done\n",
      "Iteration 105 Done\n",
      "Iteration 106 Done\n",
      "Iteration 107 Done\n",
      "Iteration 108 Done\n",
      "Iteration 109 Done\n",
      "Iteration 110 Done\n",
      "Iteration 111 Done\n",
      "Iteration 112 Done\n",
      "Iteration 113 Done\n",
      "Iteration 114 Done\n",
      "Iteration 115 Done\n",
      "Iteration 116 Done\n",
      "Iteration 117 Done\n",
      "Iteration 118 Done\n",
      "Iteration 119 Done\n",
      "Iteration 120 Done\n",
      "Iteration 121 Done\n",
      "Iteration 122 Done\n",
      "Iteration 123 Done\n",
      "Iteration 124 Done\n",
      "Iteration 125 Done\n",
      "Iteration 126 Done\n",
      "Iteration 127 Done\n",
      "Iteration 128 Done\n",
      "Iteration 129 Done\n",
      "Iteration 130 Done\n",
      "Iteration 131 Done\n",
      "Iteration 132 Done\n",
      "Iteration 133 Done\n",
      "Iteration 134 Done\n",
      "Iteration 135 Done\n",
      "Iteration 136 Done\n",
      "Iteration 137 Done\n",
      "Iteration 138 Done\n",
      "Iteration 139 Done\n",
      "Iteration 140 Done\n",
      "Iteration 141 Done\n",
      "Iteration 142 Done\n",
      "Iteration 143 Done\n",
      "Iteration 144 Done\n",
      "Iteration 145 Done\n",
      "Iteration 146 Done\n",
      "Iteration 147 Done\n",
      "Iteration 148 Done\n",
      "Iteration 149 Done\n",
      "Iteration 150 Done\n",
      "Iteration 151 Done\n",
      "Iteration 152 Done\n",
      "Iteration 153 Done\n",
      "Iteration 154 Done\n",
      "Iteration 155 Done\n",
      "Iteration 156 Done\n",
      "Iteration 157 Done\n",
      "Iteration 158 Done\n",
      "Iteration 159 Done\n",
      "Iteration 160 Done\n",
      "Iteration 161 Done\n",
      "Iteration 162 Done\n",
      "Iteration 163 Done\n",
      "Iteration 164 Done\n",
      "Iteration 165 Done\n",
      "Iteration 166 Done\n",
      "Iteration 167 Done\n",
      "Iteration 168 Done\n",
      "Iteration 169 Done\n",
      "National ChampionshipsTourney Conference ChampSRSPowerConfStealsReboundsSOSLocationPPGAConference ChampAPGSeedPPGTourney AppearancesWins: 0.764933784165\n",
      "TOPStealsPowerConfSeedAPGLocationSRSPPGNational ChampionshipsSOSTourney Conference ChampConference ChampTourney AppearancesRebounds3PGWins: 0.764711890673\n",
      "PPGTOPReboundsSOSAPGPPGAStealsTourney Conference ChampSRSTourney AppearancesSeedNational ChampionshipsLocationConference ChampWins: 0.764504085658\n",
      "SOSTourney AppearancesConference ChampStealsSeedPowerConfLocationWins: 0.764468864469\n",
      "PPGTourney Conference ChampSRSSOSConference ChampNational ChampionshipsReboundsLocationSeedAPGTourney AppearancesPPGAPowerConfSteals3PGTOPWins: 0.764317413356\n",
      "Tourney AppearancesNational ChampionshipsTOPSeed3PGPPGReboundsSOSSRSPPGALocationTourney Conference ChampPowerConfWins: 0.764137785292\n",
      "3PGStealsLocationPowerConfNational ChampionshipsTourney Conference ChampConference ChampSeedSRSPPGSOSTourney AppearancesReboundsWins: 0.764130741054\n",
      "SRSTourney Conference ChampSeedAPGSOSPPGATOPConference ChampReboundsLocationStealsNational Championships3PGWins: 0.764095519865\n",
      "SOSPPGSRSTOPNational ChampionshipsReboundsConference ChampTourney Conference ChampAPGSeedTourney AppearancesStealsPPGAPowerConfLocationWins: 0.764042688081\n",
      "PowerConfStealsSeedSRSReboundsPPGPPGANational ChampionshipsLocationSOSTOPTourney Conference ChampWins: 0.764039165962\n",
      "PPGConference ChampSeedSOSLocationAPG3PGTourney Conference ChampPowerConfStealsWins: 0.764035643843\n",
      "TOPLocation3PGConference ChampSRSStealsTourney Conference ChampPowerConfTourney AppearancesSOSPPGAReboundsAPGWins: 0.763986334179\n",
      "SOSAPGPowerConfTourney Conference ChampTourney AppearancesSRSStealsReboundsLocationSeed3PGPPGAConference ChampTOPPPGWins: 0.76398281206\n",
      "PPGAPPGLocationAPGNational ChampionshipsTOP3PGSeedSOSSRSTourney Conference ChampReboundsStealsConference ChampWins: 0.763968723584\n",
      "PPGTOPLocationStealsConference ChampSeedSOSNational ChampionshipsTourney AppearancesPPGAWins: 0.763841927303\n",
      "ReboundsPPGSOSSeedConference ChampStealsLocation3PGTOPTourney AppearancesWins: 0.763827838828\n",
      "SRSAPGPPGPowerConfTourney Conference Champ3PGStealsSeedSOSTourney AppearancesLocationTOPConference ChampNational ChampionshipsReboundsPPGAWins: 0.763612989575\n",
      "National ChampionshipsLocationSRSTOPConference ChampPPGSOSAPGPowerConfReboundsWins: 0.763612989575\n",
      "APGSOSStealsTourney Conference Champ3PGPPGALocationReboundsPowerConfSeedTOPSRSTourney AppearancesNational ChampionshipsWins: 0.763598901099\n",
      "APGPowerConfSeedStealsTOPNational ChampionshipsSRSReboundsPPGTourney AppearancesConference ChampLocationTourney Conference ChampSOSPPGAWins: 0.763503803888\n",
      "PPGAConference ChampPowerConfLocationSOSTOPTourney Conference ChampSRSStealsWins: 0.763493237532\n",
      "TOPSOS3PGPowerConfLocationPPGSeedPPGATourney AppearancesTourney Conference ChampNational ChampionshipsAPGSRSConference ChampWins: 0.763461538462\n",
      "Tourney Conference ChampTourney AppearancesLocationConference ChampPPGStealsReboundsSeedSOSNational ChampionshipsPPGAAPGTOP3PGPowerConfSRSWins: 0.763422795154\n",
      "PowerConfStealsTourney AppearancesConference ChampLocationReboundsAPGTourney Conference ChampTOPSeed3PGSOSSRSPPGWins: 0.763362919132\n",
      "Tourney AppearancesPPGAAPG3PGConference ChampSOSLocationReboundsPPGStealsPowerConfTourney Conference ChampSRSSeedWins: 0.763345308538\n",
      "PPGASOSStealsConference ChampLocationSRSPPGTourney Conference ChampAPG3PGReboundsWins: 0.763345308538\n",
      "ReboundsStealsSOSTOPConference ChampTourney Conference ChampSeedTourney AppearancesPowerConfLocationAPGWins: 0.763317131586\n",
      "SRSNational Championships3PGSeedTourney Conference ChampPPGTOPPPGAReboundsStealsSOSPowerConfWins: 0.763295998873\n",
      "APGTOPStealsSOSPPGPowerConfTourney Conference ChampSeedSRSPPGANational ChampionshipsTourney AppearancesWins: 0.763295998873\n",
      "National Championships3PGStealsSOSTOPConference ChampTourney Conference ChampAPGPPGALocationSeedWins: 0.763267821922\n",
      "TOPTourney Conference ChampLocationPPGReboundsConference ChampSOSNational ChampionshipsSeedAPGPowerConfWins: 0.763250211327\n",
      "TOPAPGTourney Conference ChampPPGLocation3PGSRSReboundsPPGASOSSeedPowerConfStealsConference ChampNational ChampionshipsWins: 0.763250211327\n",
      "SOSLocationSRS3PGNational ChampionshipsStealsTourney AppearancesAPGWins: 0.763214990138\n",
      "PPGTOPTourney Conference ChampPPGAReboundsSeedSOSLocationAPGWins: 0.763179768949\n",
      "APGSOSTOPLocationWins: 0.76317624683\n",
      "SeedTourney AppearancesConference ChampSRSSOSTOPLocationTourney Conference ChampStealsNational ChampionshipsPPGAWins: 0.763165680473\n",
      "SRSTourney Conference ChampLocationPPGAAPGSOSTOPReboundsPPGNational ChampionshipsTourney AppearancesPowerConfWins: 0.763162158354\n",
      "3PGSeedSOSAPGReboundsStealsPPGATourney Conference ChampTOPNational ChampionshipsLocationTourney AppearancesConference ChampPPGSRSWins: 0.763074105382\n",
      "SRSStealsPPGAPPG3PGLocationSOSPowerConfTourney Conference ChampWins: 0.763035362074\n",
      "SRSTOP3PGPPGAPGPPGAReboundsStealsTourney Conference ChampTourney AppearancesNational ChampionshipsLocationSOSWins: 0.763017751479\n",
      "PPGTourney Conference ChampSOSStealsTourney AppearancesNational ChampionshipsLocationReboundsAPG3PGWins: 0.762979008171\n",
      "APGConference ChampSeedTOPPPGStealsLocation3PGPowerConfTourney Conference ChampSOSNational ChampionshipsTourney AppearancesReboundsSRSWins: 0.762964919696\n",
      "PPGASeedTourney Conference ChampSRSPPGSOSPowerConfLocationTourney Appearances3PGWins: 0.762926176388\n",
      "TOPNational ChampionshipsLocationSRSSteals3PGAPGTourney AppearancesPPGAPowerConfSOSReboundsWins: 0.762905043674\n",
      "3PGAPGPPGPPGAPowerConfReboundsSOSTourney AppearancesTOPLocationSRSTourney Conference ChampWins: 0.762809946464\n",
      "Tourney AppearancesTOPPowerConfSeedPPGLocationSRSRebounds3PGConference ChampStealsPPGASOSWins: 0.762774725275\n",
      "SeedNational ChampionshipsTourney AppearancesTourney Conference ChampSRSTOPConference ChampSOSPPGAPowerConfPPG3PGAPGStealsReboundsLocationWins: 0.762771203156\n",
      "TOPTourney Conference ChampConference ChampSeedTourney AppearancesLocationPowerConfStealsAPGNational ChampionshipsPPGPPGASOS3PGSRSReboundsWins: 0.762714849253\n",
      "Tourney Conference ChampReboundsTourney AppearancesLocationConference ChampPPGSRS3PGTOPPowerConfSeedPPGASOSNational ChampionshipsStealsAPGWins: 0.762697238659\n",
      "Conference ChampStealsSOSSeedTOPTourney Conference ChampTourney AppearancesPPGAPGSRSReboundsWins: 0.7626303184\n",
      "National ChampionshipsConference ChampTourney Conference ChampRebounds3PGPPGPowerConfPPGALocationTOPAPGSRSSeedTourney AppearancesStealsSOSWins: 0.762612707805\n",
      "3PGPowerConfTourney Conference ChampReboundsPPGASOSNational ChampionshipsLocationWins: 0.76256339814\n",
      "Tourney Conference ChampReboundsTourney AppearancesTOPLocationSOS3PGSeedPowerConfWins: 0.762549309665\n",
      "StealsTourney Conference ChampSeed3PGSOSPPGPowerConfPPGAAPGTourney AppearancesTOPReboundsConference ChampNational ChampionshipsLocationSRSWins: 0.762545787546\n",
      "ReboundsLocationSOSAPGPPGStealsSRSConference ChampNational ChampionshipsWins: 0.762440123979\n",
      "PowerConfNational ChampionshipsPPG3PGPPGAAPGSeedConference ChampReboundsSOSSRSStealsTourney AppearancesTOPTourney Conference ChampLocationWins: 0.762433079741\n",
      "PPGReboundsSOSTourney Conference ChampConference ChampPowerConfNational ChampionshipsStealsAPG3PGPPGASRSSeedLocationTOPWins: 0.762390814314\n",
      "SeedSRSPowerConfPPGSOSRebounds3PGWins: 0.762320371936\n",
      "LocationSOSConference ChampSeed3PGPowerConfTourney AppearancesSRSAPGReboundsStealsTourney Conference ChampNational ChampionshipsPPGAPPGTOPWins: 0.76230628346\n",
      "Conference ChampSRSStealsLocationTOPPPGSOSTourney Conference ChampWins: 0.76221118625\n",
      "ReboundsTOPPPGAPPGTourney AppearancesConference ChampNational ChampionshipsSeedStealsPowerConfSOSWins: 0.762204142012\n",
      "StealsSRSAPGSeedReboundsNational ChampionshipsTOPConference ChampPowerConf3PGPPGATourney Conference ChampSOSWins: 0.762056213018\n",
      "APG3PGStealsSRSConference ChampTourney Conference ChampTOPPPGASOSWins: 0.762024513948\n",
      "National ChampionshipsSOSPPGAReboundsConference ChampPPGPowerConfStealsSeed3PGTourney AppearancesTourney Conference ChampTOPSRSWins: 0.762020991829\n",
      "SeedPowerConfSOS3PGPPGATOPConference ChampTourney Conference ChampStealsWins: 0.76191885038\n",
      "SOS3PGTOPAPGTourney Conference ChampSeedSRSPowerConfWins: 0.761904761905\n",
      "National Championships3PGSeedReboundsTOPTourney AppearancesTourney Conference ChampSRSPowerConfPPGPPGAAPGSOSStealsConference ChampWins: 0.761858974359\n",
      "National ChampionshipsReboundsAPGSOSPowerConfSeedConference ChampTOPTourney AppearancesPPGAWins: 0.761732178078\n",
      "National ChampionshipsSOSSRS3PGAPGStealsWins: 0.761665257819\n",
      "National Championships3PGTourney Conference ChampSeedLocationReboundsPPGATOPPowerConfStealsConference ChampTourney AppearancesPPGAPGSRSWins: 0.761640602987\n",
      "Tourney AppearancesPPGPPGASOSConference ChampSRSSeedPowerConfWins: 0.761527895182\n",
      "SOSPPGAAPGTOPStealsTourney AppearancesNational ChampionshipsRebounds3PGWins: 0.761485629755\n",
      "Conference ChampPPGAReboundsTOPTourney Conference ChampNational ChampionshipsSOSPPGSRSStealsWins: 0.761475063398\n",
      "PPGAStealsPowerConfReboundsTourney Appearances3PGSeedTOPLocationAPGSRSConference ChampPPGWins: 0.761446886447\n",
      "SOSTourney Conference ChampTOPPPGConference ChampPPGAWins: 0.761394054663\n",
      "SeedSOSPowerConfTOPAPGNational ChampionshipsWins: 0.761182727529\n",
      "SOSSeedPPGWins: 0.760770639617\n",
      "SOSAPGTourney Conference ChampReboundsPPGPPGATOPTourney AppearancesConference ChampNational ChampionshipsPowerConfSRSStealsWins: 0.760745984784\n",
      "PPGASeedPowerConfTourney Conference Champ3PGLocationNational ChampionshipsAPGStealsSRSConference ChampWins: 0.760731896309\n",
      "ReboundsPPGTourney AppearancesSOSNational ChampionshipsSeedPPGAConference ChampWins: 0.76047125951\n",
      "SOSWins: 0.760418427726\n",
      "Tourney Conference ChampTOPLocation3PGNational ChampionshipsTourney AppearancesSRSPPGAConference ChampPowerConfPPGSeedWins: 0.760344463229\n",
      "Tourney AppearancesTourney Conference ChampTOPNational ChampionshipsAPGConference ChampStealsReboundsLocationPPGPowerConfSRSWins: 0.760316286278\n",
      "PPGNational ChampionshipsReboundsTourney Conference ChampTOPTourney AppearancesAPGPowerConfSeedConference ChampStealsLocation3PGSRSWins: 0.760284587208\n",
      "SOSReboundsSeedPPGAPowerConf3PGTourney Conference ChampWins: 0.760217666948\n",
      "3PGTOPSOSSRSTourney Conference ChampWins: 0.76017892364\n",
      "National ChampionshipsTOPSOSPPGTourney Conference ChampPowerConf3PGConference ChampAPGWins: 0.760126091857\n",
      "SOSPowerConfTourney Conference ChampWins: 0.75989010989\n",
      "SRSTourney AppearancesTourney Conference ChampConference Champ3PGPPGPowerConfTOPLocationNational ChampionshipsAPGWins: 0.759685826994\n",
      "Tourney Conference ChampStealsAPGConference ChampPowerConfPPGSRSPPGA3PGWins: 0.759661172161\n",
      "LocationPPGAAPGStealsPPGSRSTourney Conference ChampTourney Appearances3PGReboundsTOPSeedNational ChampionshipsConference ChampWins: 0.759432234432\n",
      "PPGAConference ChampNational ChampionshipsTourney Conference ChampLocationPPGSRSStealsWins: 0.7594075796\n",
      "Tourney Conference ChampLocationPPGAPG3PGSRSConference ChampStealsTourney AppearancesNational ChampionshipsWins: 0.759323048746\n",
      "Tourney AppearancesReboundsSeed3PGLocationTOPNational ChampionshipsStealsSRSAPGTourney Conference ChampWins: 0.759111721612\n",
      "PowerConfPPGConference ChampSRSPPGA3PGWins: 0.759111721612\n",
      "ReboundsPowerConfTourney AppearancesSeedNational ChampionshipsLocation3PGSRSTOPAPGTourney Conference ChampPPGAWins: 0.758981403212\n",
      "PPGPowerConfPPGASeedStealsReboundsTourney Conference Champ3PGTOPConference ChampTourney AppearancesSRSWins: 0.75885812905\n",
      "APGSeedLocationTOPSRSPPGAWins: 0.758667934629\n",
      "PPGASRSTOPNational ChampionshipsPPGConference ChampRebounds3PGWins: 0.757967032967\n",
      "APGSRSWins: 0.757889546351\n",
      "3PGTOPReboundsTourney Conference ChampTourney AppearancesPowerConfAPGSRSStealsSeedWins: 0.757748661595\n",
      "SeedTourney Conference ChampSRSTOP3PGStealsConference ChampWins: 0.757688785573\n",
      "Conference ChampTourney AppearancesTOPAPGSRSWins: 0.757639475909\n",
      "LocationSRSTourney AppearancesWins: 0.757544378698\n",
      "SRSAPGTOPTourney Conference ChampRebounds3PGPPGTourney AppearancesWins: 0.757364750634\n",
      "3PGTourney Conference ChampConference ChampTOPPPGSRSLocationWins: 0.757354184277\n",
      "PowerConfSRS3PGConference ChampWins: 0.757149901381\n",
      "SRSNational ChampionshipsTourney AppearancesStealsAPGTOPWins: 0.757009016624\n",
      "StealsSRSWins: 0.756491265145\n",
      "APGStealsTourney Conference ChampNational ChampionshipsReboundsPPGSeedConference ChampTOP3PGPowerConfLocationTourney AppearancesWins: 0.754775993238\n",
      "SeedPowerConfLocationTourney Conference ChampTOP3PGStealsWins: 0.754392082277\n",
      "SeedPowerConfTOPLocationTourney AppearancesTourney Conference ChampWins: 0.753990560721\n",
      "Conference ChampPPGReboundsPowerConfLocationNational ChampionshipsWins: 0.753680614258\n",
      "PPGConference ChampLocationTOPStealsPowerConfWins: 0.753525641026\n",
      "SeedPPGAPPGPowerConfWins: 0.752363341786\n",
      "National ChampionshipsSeedTOPPowerConfTourney Conference ChampWins: 0.752331642716\n",
      "APGPowerConfTourney Conference ChampTourney AppearancesTOPWins: 0.752183713722\n",
      "PowerConfTourney Conference ChampConference ChampTOPWins: 0.75193364328\n",
      "TOPPowerConfAPGWins: 0.751655395886\n",
      "Tourney AppearancesReboundsNational Championships3PGSeedPowerConfAPGWins: 0.751644829529\n",
      "Conference ChampPowerConfAPGPPGAWins: 0.751412369682\n",
      "APGTOPNational ChampionshipsConference ChampPPG3PGPowerConfWins: 0.751208086785\n",
      "PowerConfWins: 0.751169343477\n",
      "PPGTourney AppearancesReboundsPowerConfConference ChampWins: 0.7507079459\n",
      "APGPowerConfPPGNational ChampionshipsConference ChampWins: 0.750489574528\n",
      "PPGTOPPowerConfTourney AppearancesWins: 0.750218371372\n",
      "APGStealsTourney AppearancesTOPLocationWins: 0.748559453367\n",
      "Tourney AppearancesConference ChampPPGAReboundsTOPAPGLocationStealsWins: 0.748489010989\n",
      "TOPTourney Appearances3PGStealsPPGNational ChampionshipsTourney Conference ChampWins: 0.748267117498\n",
      "Tourney AppearancesSeedAPGStealsPPG3PGTOPWins: 0.748214285714\n",
      "SeedConference ChampWins: 0.74793956044\n",
      "APGSeedReboundsWins: 0.747812764159\n",
      "SeedWins: 0.747696534235\n",
      "APG3PGConference ChampStealsSeedWins: 0.747365455058\n",
      "TOPAPGTourney AppearancesWins: 0.746978021978\n",
      "SeedAPGStealsPPGAWins: 0.746259509721\n",
      "Tourney Appearances3PGWins: 0.74623133277\n",
      "PPGATourney AppearancesWins: 0.745886165117\n",
      "PPGTourney AppearancesStealsTourney Conference ChampReboundsWins: 0.74587559876\n",
      "PPGALocationNational ChampionshipsWins: 0.745146520147\n",
      "Conference ChampPPGANational ChampionshipsWins: 0.745023245985\n",
      "Conference ChampWins: 0.74471682164\n",
      "PPGANational ChampionshipsAPGTourney Conference ChampTOPConference ChampWins: 0.7445231051\n",
      "TOPPPGA3PGLocationWins: 0.744449140603\n",
      "PPGAAPGStealsWins: 0.744332910679\n",
      "APGReboundsWins: 0.744170893209\n",
      "Wins: 0.743832769794\n",
      "Conference ChampTourney Conference ChampStealsWins: 0.743818681319\n",
      "3PGTOPWins: 0.7438151592\n",
      "StealsPPGAWins: 0.743794026486\n",
      "ReboundsPPGTourney Conference ChampTOPWins: 0.74378346013\n",
      "PPGAStealsAPGLocationPPGWins: 0.743448858833\n",
      "APGWins: 0.743417159763\n",
      "ReboundsAPGWins: 0.743388982812\n",
      "3PGTourney Conference ChampWins: 0.743262186531\n",
      "TOPWins: 0.742906452522\n",
      "ReboundsConference ChampTOP3PGWins: 0.742469709777\n",
      "StealsWins: 0.742110453649\n"
     ]
    }
   ],
   "source": [
    "trainDict={}\n",
    "xTrain = df.as_matrix()\n",
    "for i in range(17):   \n",
    "    for p in range(10):\n",
    "        modifiedCategories = np.random.choice(categories,i, replace=False)\n",
    "        modifiedCategories = np.append(modifiedCategories, 'Wins')\n",
    "        modCatList = modifiedCategories.tolist()\n",
    "        str1 = ''.join(modCatList)\n",
    "        modifiedxTrain = df[modifiedCategories].as_matrix()\n",
    "        #model = tree.DecisionTreeClassifier()\n",
    "        #model = tree.DecisionTreeRegressor()\n",
    "        #model = linear_model.LogisticRegression()\n",
    "        #model = linear_model.BayesianRidge()\n",
    "        #model = linear_model.Lasso()\n",
    "        #model = svm.SVC()\n",
    "        #model = svm.SVR()\n",
    "        #model = linear_model.Ridge(alpha = 0.5)\n",
    "        #model = AdaBoostClassifier(n_estimators=100)\n",
    "        #model1 = GradientBoostingClassifier(n_estimators=100)\n",
    "        model = GradientBoostingRegressor(n_estimators=100)\n",
    "        #model = RandomForestClassifier(n_estimators=200)\n",
    "        #model = KNeighborsClassifier(n_neighbors=101)\n",
    "        accuracy=[]\n",
    "        for q in range(10):\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(modifiedxTrain, yTrain)\n",
    "            #X_train, X_test, Y_train, Y_test = train_test_split(xTrain, yTrain)\n",
    "            model.fit(X_train, Y_train)\n",
    "            results = model.fit(X_train, Y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            preds[preds < .5] = 0\n",
    "            preds[preds >= .5] = 1\n",
    "            accuracy.append(np.mean(preds == Y_test))\n",
    "        trainDict[str1] = sum(accuracy)/len(accuracy)\n",
    "        print 'Iteration',(i*10 + p),'Done'\n",
    "for key, value in sorted(trainDict.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "    print \"%s: %s\" % (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765101938082\n"
     ]
    }
   ],
   "source": [
    "categories=['Wins','PPG','PPGA','PowerConf','3PG', 'Conference Champ', 'Tourney Conference Champ',\n",
    "            'Seed','SOS','SRS', 'Rebounds', 'Steals', 'Tourney Appearances','National Championships','Location']\n",
    "modifiedxTrain = df[categories].as_matrix()\n",
    "modifiedxTrain = modifiedxTrain[50000:, :]\n",
    "yTrain = yTrain[50000:]\n",
    "model = GradientBoostingRegressor(n_estimators=100)\n",
    "accuracy=[]\n",
    "for q in range(1):\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(modifiedxTrain, yTrain)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(modifiedxTrain, yTrain)\n",
    "    model.fit(X_train, Y_train)\n",
    "    results = model.fit(X_train, Y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    preds[preds < .5] = 0\n",
    "    preds[preds >= .5] = 1\n",
    "    accuracy.append(np.mean(preds == Y_test))\n",
    "print sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
